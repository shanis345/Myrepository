{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKsRDH5ZUdfasdv"
      },
      "source": [
        "# Assignment 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkkLOLDN1v1d"
      },
      "source": [
        "### <span style=\"color:chocolate\"> Submission requirements </span>\n",
        "\n",
        "Your work will not be graded if your notebook doesn't include output. In other words, <span style=\"color:red\"> make sure to rerun your notebook before submitting to Gradescope </span> (Note: if you are using Google Colab: go to Edit > Notebook Settings  and uncheck Omit code cell output when saving this notebook, otherwise the output is not printed).\n",
        "\n",
        "Additional points may be deducted if these requirements are not met:\n",
        "\n",
        "    \n",
        "* Comment your code;\n",
        "* Each graph should have a title, labels for each axis, and (if needed) a legend. Each graph should be understandable on its own;\n",
        "* Try and minimize the use of the global namespace (meaning, keep things inside functions).\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QHsj-Dt1v1e"
      },
      "source": [
        "### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7X58hOMTUH-w"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns  # for nicer plots\n",
        "sns.set(style=\"darkgrid\")  # default style\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import metrics\n",
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "tf.get_logger().setLevel('INFO')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gunG5_gk1v1g"
      },
      "source": [
        "---\n",
        "### Step 1: Data ingestion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHLcriKWLRe4"
      },
      "source": [
        "You'll train a binary classifier using the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset. This consists of 70,000 grayscale images (28x28). Each image is associated with 1 of 10 classes. The dataset was split by the creators; there are 60,000 training images and 10,000 test images. Note also that Tensorflow includes a growing [library of datasets](https://www.tensorflow.org/datasets/catalog/overview) and makes it easy to load them in numpy arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8NEdCwi1v1g"
      },
      "outputs": [],
      "source": [
        "# Load the Fashion MNIST dataset.\n",
        "(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GJi7gLZ1v1h"
      },
      "source": [
        "---\n",
        "### Step 2: Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgDFvHo31v1h"
      },
      "source": [
        "Exploratory Data Analysis (EDA) and Data Preprocessing are often iterative processes that involve going back and forth to refine and improve the quality of data analysis and preparation. However, the specific order can vary depending on the project's requirements. In some cases, starting with EDA, as you see in this assignment, could be more useful, but there is no rigid rule dictating the sequence in all situations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJYR20i51v1h"
      },
      "source": [
        "### <span style=\"color:chocolate\">Exercise 1:</span> Getting to know your data (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load_auto_data_set_code"
      },
      "source": [
        "Complete the following tasks:\n",
        "\n",
        "1. Print the shapes and types of (X_train, Y_train) and (X_test, Y_test). Interpret the shapes (i.e., what do the numbers represent?). Hint: For types use the <span style=\"color:chocolate\">type()</span> function.\n",
        "2. Define a list of strings of class names corresponding to each class in (Y_train, Y_test). Call this list label_names. Hint: Refer to the Fashion MNIST documentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsofjOep1v1i"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "# Print the shapes and types of (X_train, Y_train) and (X_test, Y_test)\n",
        "print(\"The shape of X_train is \", X_train.shape, \" and the type of X_train is \", type(X_train))\n",
        "print(\"The shape of Y_train is \", Y_train.shape, \" and the type of Y_train is \", type(Y_train))\n",
        "print(\"The shape of X_test is \", X_test.shape, \" and the type of X_test is \", type(X_test))\n",
        "print(\"The shape of Y_test is \", Y_test.shape, \" and the type of Y_test is \", type(Y_test))\n",
        "\n",
        "# Define a list of strings of class names\n",
        "label_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjCXv7GQ1v1i"
      },
      "source": [
        "### <span style=\"color:chocolate\">Exercise 2:</span> Getting to know your data - cont'd (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPGrBXt21v1i"
      },
      "source": [
        "Fashion MNIST images have one of 10 possible labels (shown above)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQlLLXAm1v1i"
      },
      "source": [
        "Complete the following tasks:\n",
        "\n",
        "1. Display the first 5 images in X_train for each class in Y_train, arranged in a 10x5 grid. Use the label_names list defined above;\n",
        "2. Determine the minimum and maximum pixel values for images in the X_train dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7f7RcHZq1v1i"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "fig, axes = plt.subplots(10, 5, figsize=(10, 18))\n",
        "fig.subplots_adjust(hspace=0.5)\n",
        "\n",
        "# Display the first 5 images for each class\n",
        "for class_label in range(10):\n",
        "    index = np.where(Y_train == class_label)[0]\n",
        "    for i in range(5):\n",
        "        ax = axes[class_label, i]\n",
        "        ax.imshow(X_train[index[i]])\n",
        "plt.show()\n",
        "\n",
        "# Determine the minimum and maximum pixel values for images in the X_train dataset\n",
        "print(\"Minimum pixel value: \", np.min(X_train))\n",
        "print(\"Maximum pixel value: \", np.max(X_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZpFxxStzSrP"
      },
      "source": [
        "---\n",
        "### Step 3: Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wy2lg9F61v1j"
      },
      "source": [
        "This step is essential for preparing this image data in a format that is suitable for ML algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlmB87_g1v1j"
      },
      "source": [
        "### <span style=\"color:chocolate\">Exercise 3:</span> Feature preprocessing (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8pZj0iG1v1j"
      },
      "source": [
        "In the previous lab, the input data had just a few features. Here, we treat **every pixel value as a separate feature**, so each input example has 28x28 (784) features!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZTEM1831v1j"
      },
      "source": [
        "In this exercise, you'll perform the following tasks:\n",
        "\n",
        "1. Normalize the pixel values in both X_train and X_test data so they range between 0 and 1;\n",
        "2. For each image in X_train and X_test, flatten the 2-D 28x28 pixel array to a 1-D array of size 784. Hint: use the <span style=\"color:chocolate\">reshape()</span> method available in NumPy. Note that by doing so you will overwrite the original arrays;\n",
        "3. Pint the shape of X_train and X_test arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h7Pigz8k1v1j"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "# Normalize the pixel values in both X_train and X_test data so they range between 0 and 1\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "# Flatten the 2D pixel arrays to 1D array of size 784\n",
        "X_train = X_train.reshape(X_train.shape[0], 784)\n",
        "X_test = X_test.reshape(X_test.shape[0], 784)\n",
        "\n",
        "# Print the shape of X_train and X_test arrays\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2U94dxFv1v1j"
      },
      "source": [
        "### <span style=\"color:chocolate\">Exercise 4:</span> Label preprocessing (5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgzxzdWh1v1j"
      },
      "source": [
        "This assignment involves binary classification. Specifically, the objective is to predict whether an image belongs to the sneaker class (class 7) or not.\n",
        "\n",
        "Therefore, write code so that for each example in (Y_train, Y_test), the outcome variable is represented as follows:\n",
        "* $y=1$, for sneaker class (positive examples), and\n",
        "* $y=0$, for non-sneaker class (negative examples).\n",
        "\n",
        "Note: To avoid \"ValueError: assignment destination is read-only\", first create a copy of the (Y_train, Y_test) data and call the resulting arrays (Y_train, Y_test). Then overwrite the (Y_train, Y_test) arrays to create binary outcomes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rycefc7D1v1j"
      },
      "outputs": [],
      "source": [
        "# Make copies of the original dataset for binary classification task.\n",
        "Y_train = np.copy(Y_train)\n",
        "Y_test = np.copy(Y_test)\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Create binary outcome: 1 for sneaker (class 7), 0 for non-sneaker classes\n",
        "Y_train = np.where(Y_train == 7, 1, 0)\n",
        "Y_test = np.where(Y_test == 7, 1, 0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQKK2xe71v1j"
      },
      "source": [
        "### <span style=\"color:chocolate\">Exercise 5:</span> Data splits (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ComKoU351v1k"
      },
      "source": [
        "Using the <span style=\"color:chocolate\">train_test_split()</span> method available in scikit-learn:\n",
        "1. Retain 20% from the training data for validation purposes. Set random state to 1234. All the other arguments of the method are set to default values. Name the resulting dataframes as follows: X_train_mini, X_val, Y_train_mini, Y_val.\n",
        "2. Print the shape of each array."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5lF2bfw1v1k"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "X_train_mini, X_val, Y_train_mini, Y_val = train_test_split(X_train, Y_train, test_size = 0.2, random_state= 1234)\n",
        "\n",
        "print(\"Shape of X_train_mini: \", X_train_mini.shape)\n",
        "print(\"Shape of X_val: \", X_val.shape)\n",
        "print(\"Shape of Y_train_mini: \", Y_train_mini.shape)\n",
        "print(\"Shape of Y_val: \", Y_val.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "np6EB_by1v1k"
      },
      "source": [
        "### <span style=\"color:chocolate\">Exercise 6:</span> Data shuffling (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iR1GuGIO1v1k"
      },
      "source": [
        "Since you'll be using Batch Gradient Descent (BGD) for training, it is important that **each batch is a random sample of the data** so that the gradient computed is representative.\n",
        "\n",
        "1. Use [integer array indexing](https://numpy.org/doc/stable/reference/arrays.indexing.html#integer-array-indexing) to re-order (X_train_mini, Y_train_mini) using a list of shuffled indices. In doing so, you will overwrite the arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDiYeWrx1v1k"
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# Use np.random.permutation to shuffle indices\n",
        "# I couldn't get access to the hyperlink so I took the method used from the previous assignment\n",
        "shuffled_indices = np.random.permutation(len(X_train_mini))\n",
        "\n",
        "# Use these indices to re-order the training data arrays\n",
        "X_train_mini = X_train_mini[shuffled_indices]\n",
        "Y_train_mini = Y_train_mini[shuffled_indices]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPVMCqIo1v1k"
      },
      "source": [
        "---\n",
        "### Step 4: Exploratory Data Analysis (EDA) - cont'd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPMlEj-61v1k"
      },
      "source": [
        "Before delving into model training, let's further explore the raw feature values by comparing sneaker and non-sneaker training images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIevcLNP1v1k"
      },
      "source": [
        "### <span style=\"color:chocolate\">Exercise 7:</span> Pixel distributions (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8HXu4fy1v1k"
      },
      "source": [
        "1. Identify all sneaker images in X_train_mini and calculate the mean pixel value for each sneaker image. Visualize these pixel values using a histogram. Print the mean pixel value across all sneaker images.\n",
        "2. Identify all non-sneaker images in X_train_mini and calculate the mean pixel value for each non-sneaker image. Visualize these pixel values using a histogram. Print the mean pixel value across all non-sneaker images.\n",
        "3. Based on the histogram results, assess whether there is any evidence suggesting that pixel values can be utilized to distinguish between sneaker and non-sneaker images. Justify your response.\n",
        "\n",
        "Notes: Make sure to provide a descriptive title and axis labels for each histogran. Make sure you utilize Y_train_mini to locate the sneaker and non-sneaker class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lsG3hO_1v1l"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "# Identify all sneaker images and calculate the mean pixel value for each sneaker image\n",
        "sneaker_indices = np.where(Y_train_mini == 1)[0]\n",
        "sneaker_means = np.mean(X_train_mini[sneaker_indices], axis=1)\n",
        "\n",
        "# Visualize sneaker mean pixel values using a histogram\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(sneaker_means, bins=30, color='blue')\n",
        "plt.title('Mean Pixel Values of Sneaker Images')\n",
        "plt.xlabel('Mean Pixel Value')\n",
        "plt.ylabel('Number of Images')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Print the mean pixel value across all sneaker images\n",
        "mean_sneaker_pixel_value = np.mean(sneaker_means)\n",
        "print(\"Mean pixel value across all sneaker images: \", mean_sneaker_pixel_value)\n",
        "\n",
        "# Identify all non-sneaker images and calculate the mean pixel value for each sneaker image\n",
        "non_sneaker_indices = np.where(Y_train_mini == 0)[0]\n",
        "non_sneaker_means = np.mean(X_train_mini[non_sneaker_indices], axis=1)\n",
        "\n",
        "# Visualize non-sneaker mean pixel values using a histogram\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(non_sneaker_means, bins=30, color='red')\n",
        "plt.title('Mean Pixel Values of Non-Sneaker Images')\n",
        "plt.xlabel('Mean Pixel Value')\n",
        "plt.ylabel('Number of Images')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Print the mean pixel value across all non-sneaker images\n",
        "mean_non_sneaker_pixel_value = np.mean(non_sneaker_means)\n",
        "print(\"Mean pixel value across all non-sneaker images: \", mean_non_sneaker_pixel_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer for Exercise 7\n",
        "Based on the histograms and mean pixel values across all images, I ran a t-test to determine whether the difference is statistically significant enough to reject the null hypothesis. The null hypothesis in this context is that there is no significant difference between the mean pixel values of sneaker images and non-sneaker images. The t-test result showed a t-statistic of -71 and a very low p-value, which led me to reject the null hypothesis. This indicates that the mean pixel values between the two groups are significantly different. While this suggests that mean pixel value could be a useful feature for distinguishing sneaker images, we need further evaluation to assess its practical effectiveness."
      ],
      "metadata": {
        "id": "hmeyGTwy4_FC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "\n",
        "# Assuming `sneaker_means` and `non_sneaker_means` are already defined\n",
        "# and contain the mean pixel values for sneaker and non-sneaker images\n",
        "\n",
        "# Perform an independent two-sample t-test\n",
        "t_stat, p_value = stats.ttest_ind(sneaker_means, non_sneaker_means)\n",
        "\n",
        "# Print the results of the t-test\n",
        "print(t_stat)\n",
        "print(p_value)"
      ],
      "metadata": {
        "id": "ai-itIe928pL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0Ds5xd91v1l"
      },
      "source": [
        "---\n",
        "### Step 4: Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a7cbZ2G1v1l"
      },
      "source": [
        "### <span style=\"color:chocolate\">Exercise 8:</span> Baseline model (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGgQ-ASYiHbK"
      },
      "source": [
        "When dealing with classification problems, a simple baseline is to select the *majority* class (the most common label in the training set) and use it as the prediction for all inputs.\n",
        "\n",
        "With this information in mind:\n",
        "\n",
        "1. What is the number of sneaker images in Y_train_mini?\n",
        "2. What is the number of non-sneaker images in Y_train_mini?\n",
        "3. What is the majority class in Y_train_mini?\n",
        "4. What is the accuracy of a majority class classifier for Y_train_mini?\n",
        "5. Implement a function that computes the Log Loss (binary cross-entropy) metric and use it to evaluate this baseline on both the mini train (Y_train_mini) and validation (Y_val) data. Use 0.1 as the predicted probability for your baseline (reflecting what we know about the original distribution of classes in the mini training data). Hint: for additional help, see the file ``04 Logistic Regression with Tensorflow_helpers.ipynb``."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The number of sneaker/non-sneaker images in Y_train_mini\n",
        "num_sneakers = np.sum(Y_train_mini == 1)\n",
        "num_non_sneakers = np.sum(Y_train_mini == 0)\n",
        "\n",
        "print(\"Number of sneaker images in Y_train_mini: \", num_sneakers)\n",
        "print(\"Number of non-sneaker iamges in Y_train_mini: \", num_non_sneakers)"
      ],
      "metadata": {
        "id": "UJC0BbII8z4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is the majority class in Y_train_mini?\n",
        "For the sneaker class, the majority class is 0 (non-sneaker). 90% of the data are non-sneakers and only the remaining 10% are sneaker images"
      ],
      "metadata": {
        "id": "ElI34fQT84AN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is the accuracy of a majority class classifier for Y_train_mini?\n",
        "Written in the insturction, a simple baseline is to select the majority class (the most common label in the training set) and use it as the prediction for all inputs. Regarding this method, the accuracy of a majority class classifier would be 90%. (If we classify all images as non-sneakers, it would show 90% accuracy)"
      ],
      "metadata": {
        "id": "l6kIR5TX9b1G"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMKuRq7g1v1o"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "# Define the sigmoid function\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "# Define the log loss function\n",
        "def log_loss(y_true, y_pred):\n",
        "    loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "    return loss\n",
        "\n",
        "# Use the log_loss function to calculate the log loss and print it (predicted probability; 0.1)\n",
        "loss = log_loss(Y_train_mini, 0.1)\n",
        "print(\"Log Loss on training data with predicted probability as 0.1: \", loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "omkK2wxw1v1p"
      },
      "source": [
        "### <span style=\"color:chocolate\">Exercise 9:</span> Improvement over Baseline with TensorFlow (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jS_NMeO1v1p"
      },
      "source": [
        "Let's use TensorFlow to train a binary logistic regression model much like you did in the previous assignment. The goal here is to build a ML model to improve over the baseline classifier.\n",
        "\n",
        "1. Fill in the <span style=\"color:green\">NotImplemented</span> parts of the build_model() function below by following the instructions provided as comments. Hint: the activation function, the loss, and the evaluation metric are different compared to the linear regression model;\n",
        "2. Build and compile a model using the build_model() function and the (X_train_mini, Y_train_mini) data. Set learning_rate = 0.0001. Call the resulting object *model_tf*.\n",
        "3. Train *model_tf* using the (X_train_mini, Y_train_mini) data. Set num_epochs = 5 and batch_size=32. Pass the (X_val, Y_val) data for validation. Hint: see the documentation behind the [tf.keras.Model.fit()](https://bcourses.berkeley.edu/courses/1534588/files/88733489?module_item_id=17073646) method.\n",
        "3. Generate a plot (for the mini training and validation data) with the loss values on the y-axis and the epoch number on the x-axis for visualization. Make sure to include axes name and title. Hint: check what the [tf.keras.Model.fit()](https://bcourses.berkeley.edu/courses/1534588/files/88733489?module_item_id=17073646) method returns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o76FGxvi1v1p"
      },
      "outputs": [],
      "source": [
        "def build_model(num_features, learning_rate):\n",
        "  \"\"\"Build a TF linear regression model using Keras.\n",
        "\n",
        "  Args:\n",
        "    num_features: The number of input features.\n",
        "    learning_rate: The desired learning rate for SGD.\n",
        "\n",
        "  Returns:\n",
        "    model: A tf.keras model (graph).\n",
        "  \"\"\"\n",
        "  # This is not strictly necessary, but each time you build a model, TF adds\n",
        "  # new nodes (rather than overwriting), so the colab session can end up\n",
        "  # storing lots of copies of the graph when you only care about the most\n",
        "  # recent. Also, as there is some randomness built into training with SGD,\n",
        "  # setting a random seed ensures that results are the same on each identical\n",
        "  # training run.\n",
        "  tf.keras.backend.clear_session()\n",
        "  tf.random.set_seed(0)\n",
        "\n",
        "  # Build a model using keras.Sequential. While this is intended for neural\n",
        "  # networks (which may have multiple layers), we want just a single layer for\n",
        "  # binary logistic regression.\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Dense(\n",
        "      units=NotImplemented,        # output dim\n",
        "      input_shape=NotImplemented,  # input dim\n",
        "      use_bias=True,               # use a bias (intercept) param\n",
        "      activation=NotImplemented,\n",
        "      kernel_initializer=NotImplemented,  # initialize params to 1\n",
        "      bias_initializer=NotImplemented,    # initialize bias to 1\n",
        "  ))\n",
        "\n",
        "  # We need to choose an optimizer. We'll use SGD, which is actually mini-batch GD\n",
        "  optimizer = NotImplemented\n",
        "\n",
        "  # Finally, compile the model. Select the accuracy metric. This finalizes the graph for training.\n",
        "  NotImplemented\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4boo-ndy1v1p"
      },
      "outputs": [],
      "source": [
        "tf.random.set_seed(0)\n",
        "# 2. Build and compile model\n",
        "# YOUR CODE HERE\n",
        "\n",
        "# 3. Fit the model\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbIu0sAO1v1p"
      },
      "source": [
        "---\n",
        "### Step 5: Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etFWtjc01v1p"
      },
      "source": [
        "Hyperparameter tuning is a crucial step in optimizing ML models. It involves systematically adjusting hyperparameters such as learning rate, number of epochs, and optimizer to find the model configuration that leads to the best generalization performance.\n",
        "\n",
        "This tuning process is typically conducted by monitoring the model's performance on the validation vs. training set. It's important to note that using the test set for hyperparameter tuning can compromise the integrity of the evaluation process by violating the assumption of \"blindness\" of the test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9yolcjE1v1p"
      },
      "source": [
        "### <span style=\"color:chocolate\">Exercise 10:</span> Hyperparameter tuning (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_Fnnfud1v1q"
      },
      "source": [
        "1. Fine-tune the **learning rate** and **number of epochs** hyperparameters of *model_tf* to determine the setup that yields the most optimal generalization performance. Feel free to explore various values for these hyperparameters. Hint: you can manually test different hyperparameter values or you can use the [Keras Tuner](https://www.tensorflow.org/tutorials/keras/keras_tuner). If you decide to work with the Keras Tuner, define a new model building function named <span style=\"color:chocolate\">build_model_tuner()</span>.\n",
        "\n",
        "After identifying your preferred model configuration, print the following information:\n",
        "\n",
        "2. The first five learned parameters of the model (this should include the bias term);\n",
        "3. The loss at the final epoch on both the mini training and validation datasets;\n",
        "4. The percentage difference between the losses observed on the mini training and validation datasets.\n",
        "5. Compare the training/validation loss of the TensorFlow model (model_tf) with the baseline model's loss. Does the TensorFlow model demonstrate an improvement over the baseline model?\n",
        "\n",
        "\n",
        "Please note that we will consider 'optimal model configuration' any last-epoch training and validation loss that is below 0.08."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "08kDrbtt1v1q"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIBsql-L1v1q"
      },
      "source": [
        "---\n",
        "### Step 6: Evaluation and Generalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhlHtw801v1q"
      },
      "source": [
        "\n",
        "Now that you've determined the optimal set of hyperparameters, it's time to evaluate your optimized model on the test data to gauge its performance in real-world scenarios, commonly known as inference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBynLKPo1v1q"
      },
      "source": [
        "### <span style=\"color:chocolate\">Exercise 11:</span> Computing accuracy (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuoVsb9Z1v1q"
      },
      "source": [
        "1. Calculate aggregate accuracy on both mini train and test datasets using a probability threshold of 0.5. Hint: You can utilize the <span style=\"color:chocolate\">model.evaluate()</span> method provided by tf.keras. Note: Aggregate accuracy measures the overall correctness of the model across all classes in the dataset;\n",
        "\n",
        "2. Does the model demonstrate strong aggregate generalization capabilities? Provide an explanation based on your accuracy observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysNUsx9C1v1q"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpq5UkAx1v1q"
      },
      "source": [
        "### <span style=\"color:chocolate\">Exercise 12:</span> Fairness evaluation (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BizPjxW21v1q"
      },
      "source": [
        "1. Generate and visualize the confusion matrix on the test dataset using a probability threshold of 0.5. Additionally, print the True Positives (TP), False Negatives (FN), False Positives (FP), and True Negatives (TN). Hint: you can utilize the <span style=\"color:chocolate\">model.predict()</span> method available in tf.keras, and then the <span style=\"color:chocolate\">confusion_matrix()</span>, <span style=\"color:chocolate\">ConfusionMatrixDisplay()</span> methods available in sklearn.metrics;\n",
        "\n",
        "2. Compute subgroup accuracy, separately for the sneaker and non-sneaker classes, on the test dataset using a probability threshold of 0.5. Reflect on any observed accuracy differences (potential lack of fairness) between the two classes.\n",
        "\n",
        "3. Does the model demonstrate strong subgroup generalization capabilities? Provide an explanation based on your accuracy observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9wHZHg71v1r"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bQeXaoW1v1r"
      },
      "source": [
        "----\n",
        "#### <span style=\"color:chocolate\">Additional practice question</span> (not graded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMNlo2NC1v1r"
      },
      "source": [
        "Is it possible to enhance the prediction accuracy for the sneaker class by performing the following steps?\n",
        "\n",
        "1. Implement data balancing techniques, such as oversampling or undersampling, to equalize the representation of both classes.\n",
        "2. After balancing the data, retrain the model on the balanced dataset.\n",
        "3. Evaluate the model's performance, particularly focusing on the accuracy achieved for the sneaker class."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}